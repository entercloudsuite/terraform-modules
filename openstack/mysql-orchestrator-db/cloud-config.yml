#cloud-config

write_files:
  - content: |
      - name: haproxy
        hosts: 127.0.0.1
        pre_tasks:
          - name: install xinetd
            package: name=xinetd
          - name: add xinetd check
            copy:
              dest: /etc/xinetd.d/mysqlchk
              content: |
                # default: on
                # description: checkwrite
                service mysqlchk
                {
                        disable = no
                        flags = REUSE
                        socket_type = stream
                        port = 9300
                        wait = no
                        user = root
                        server = /usr/local/sbin/mysqlchk
                        log_on_failure += USERID
                        per_source = UNLIMITED
                }
          - name: add service
            lineinfile:
              path: /etc/services
              regexp: '^mysqlchk'
              line: 'mysqlchk 9300/tcp # mysqlchk for slaves'
          - name: add script mysqlchk
            copy:
              dest: /usr/local/sbin/mysqlchk
              mode: 0550
              content: |
                #!/bin/bash
                #
                # This script checks if a mysql server is healthy running on localhost. It will
                # return:
                # "HTTP/1.x 200 OK\r" (if mysql is running smoothly)
                # - OR -
                # "HTTP/1.x 500 Internal Server Error\r" (else)
                #
                # The purpose of this script is make haproxy capable of monitoring mysql properly
                #
                SLAVE_LAG_LIMIT=5
                MYSQL_HOST="localhost"
                MYSQL_PORT="${mysql_port}"
                MYSQL_USERNAME='${mysql_admin_name}'
                MYSQL_PASSWORD='${mysql_admin_password}'
                MYSQL_BIN='/usr/bin/mysql'
                MYSQL_OPTS="-q -A --connect-timeout=10"
                TMP_FILE="/dev/shm/mysqlchk.$$.out"
                ERR_FILE="/dev/shm/mysqlchk.$$.err"
                FORCE_FAIL="/dev/shm/proxyoff"

                preflight_check()
                {
                    for I in "$TMP_FILE" "$ERR_FILE"; do
                        if [ -f "$I" ]; then
                            if [ ! -w $I ]; then
                                echo -e "HTTP/1.1 503 Service Unavailable\r\n"
                                echo -e "Content-Type: Content-Type: text/plain\r\n"
                                echo -e "\r\n"
                                echo -e "Cannot write to $I\r\n"
                                echo -e "\r\n"
                                exit 1
                            fi
                        fi
                    done
                }

                return_ok()
                {
                    echo -e "HTTP/1.1 200 OK\r\n"
                    echo -e "Content-Type: text/html\r\n"
                    echo -e "Content-Length: 43\r\n"
                    echo -e "\r\n"
                    if [ $role == "master" ]; then
                        echo -e "<html><body>MySQL master is running.</body></html>\r\n"
                    elif [ $role == "slave" ]; then
                        echo -e "<html><body>MySQL slave is running. (Slave lag: $SLAVE_LAG)</body></html>\r\n"
                    else
                        echo -e "<html><body>MySQL is running.</body></html>\r\n"
                    fi
                    echo -e "\r\n"
                  #  rm $ERR_FILE $TMP_FILE
                    exit 0
                }
                return_fail()
                {
                    echo -e "HTTP/1.1 503 Service Unavailable\r\n"
                    echo -e "Content-Type: text/html\r\n"
                    echo -e "Content-Length: 42\r\n"
                    echo -e "\r\n"
                    echo -e "<html><body>MySQL is *down*.</body></html>\r\n"
                    echo -e "\r\n"
                    exit 1
                }

                preflight_check

                if [ -f "$FORCE_FAIL" ]; then
                        echo "$FORCE_FAIL found" > $ERR_FILE
                        return_fail
                fi

                CMDLINE="$MYSQL_BIN $MYSQL_OPTS --host=$MYSQL_HOST --port=$MYSQL_PORT --user=$MYSQL_USERNAME --password=$MYSQL_PASSWORD -e"
                SLAVE_IO=$($${CMDLINE} 'SHOW SLAVE STATUS' --vertical 2>/dev/null | grep Slave_IO_Running |  tail -1 | awk {'print $2'})
                SLAVE_SQL=$($${CMDLINE} 'SHOW SLAVE STATUS' --vertical 2>/dev/null | grep Slave_SQL_Running | head -1 | awk {'print $2'})

                if [[ "$${SLAVE_IO}" == "Yes" ]] && [[ "$${SLAVE_SQL}" == "Yes" ]]; then
                    role='slave'
                    SLAVE_LAG=$($${CMDLINE} 'SHOW SLAVE STATUS' --vertical 2>/dev/null | grep Seconds_Behind_Master | tail -1 | awk {'print $2'})
                    if [[ $SLAVE_LAG = 0 ]]; then
                        return_ok
                    elif [[ $SLAVE_LAG < $SLAVE_LAG_LIMIT ]] ; then
                        return_ok
                    fi
                else
                    MASTER_SLAVE_HOSTS=$($CMDLINE 'SHOW SLAVE HOSTS' 2>/dev/null | wc -l)
                    if [ $MASTER_SLAVE_HOSTS -gt 1 ]; then
                        role='master'
                        READ_ONLY=$($CMDLINE 'SHOW GLOBAL VARIABLES LIKE "read_only"' --vertical 2>/dev/null | tail -1 | awk {'print $2'})
                        [[ "$${READ_ONLY}" == "OFF" ]] && return_ok
                    fi
                fi

                return_fail

          - name: reload xinetd conf
            service: name=xinetd state=reloaded
          - name: copy haproxy conf
            local_action:
              module: copy
              content: |
                {% raw %}
                listen write-pool
                    bind *:3306
                    mode tcp
                    server {{key "mysql/master/${name}/hostname"}} {{key "mysql/master/${name}/ipv4"}}:{{key "mysql/master/${name}/port"}} check port {{key "mysql/master/${name}/port"}} init-addr last,libc,none
                {% endraw %}
              dest: master.ctmpl
          - name: stop consul
            service: name=consul state=stopped
          - name: clean /opt/consul/data/serf/local.keyring
            file: path=/opt/consul/data/serf/local.keyring state=absent
        roles:
          - role: entercloudsuite.haproxy
            haproxy_user: ${mysql_admin_name}
            haproxy_pass: ${mysql_admin_password}
            haproxy_global: |
              global
                  log /dev/log local0
                  log /dev/log local1 notice
                  chroot /var/lib/haproxy
                  stats socket /run/haproxy/admin.sock mode 660 level admin
                  stats timeout 30s
                  user haproxy
                  group haproxy
                  daemon
                  maxconn 200000
                  nbproc "{{ ansible_processor_vcpus }}"
              {% for n in range(ansible_processor_vcpus) %}
                  cpu-map {{ n + 1 }} {{ n }}
              {% endfor %}
                  ca-base /etc/ssl/certs
                  crt-base /etc/ssl/private
                  ssl-default-bind-ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS:!3DES
                  ssl-default-bind-options no-sslv3
                  tune.ssl.default-dh-param 2048
                  hard-stop-after 1s

            haproxy_conf: |
              resolvers dns-consul
                  nameserver dns consul.service.${consul_datacenter}.consul:53
                  accepted_payload_size 8192
                  hold valid 1s

              listen slaves
                  bind *:${mysql_slaves_port}
                  mode tcp
                  option tcp-check
                  tcp-check expect string is\ running.
                  balance leastconn
                  server-template mysql 0-99 ${name}.service.automium.consul:${mysql_port} check port 9300 on-marked-down shutdown-sessions init-addr last,libc,none resolvers dns-consul
          - role: entercloudsuite.consul-template
            consul_template_use_systemd: true
            consul_template_consul_server: ${consul}
            consul_template_staging_area: /var/spool
            consul_template_wait: 0s
            consul_template_template_files:
              - { src: master.ctmpl }
            consul_template_templates:
              - name: master.ctmpl
                dest: /etc/haproxy/conf.d/master.cfg
                cmd: service haproxy reload || true
                perms: 666
                backup: true
          - role: entercloudsuite.consul
            consul_config_validate: "{{ consul_user_home }}/bin/consul validate -config-format=json %s"
            consul_configs:
              main:
                bind_addr: 0.0.0.0
                client_addr: 0.0.0.0
                node_name: "{{ ansible_hostname }}"
                data_dir: "{{ consul_data_dir }}"
                encrypt: "${consul_encrypt}"
                datacenter: "${consul_datacenter}"
                enable_syslog: true
                server: false
                ui: true
                enable_script_checks: true
                services:
                  - name: "${name}"
                    checks:
                      - http: "http://${mysql_admin_name}:${mysql_admin_password}@127.0.0.1:8282"
                        method: "GET"
                        interval: "2s"
                  - name: "exporter_node"
                    port: 9100
                  - name: "exporter_haproxy"
                    port: 9101
                  - name: "exporter_mysqld"
                    port: 9104
                rejoin_after_leave: true
                retry_join:
                  - "${consul}"

      - name: format volume
        hosts: 127.0.0.1
        pre_tasks:
          - name: install thin-provisioning-tools
            package: name=thin-provisioning-tools
          - name: setup partition
            parted:
              device: /dev/vdb
              number: 1
              flags: [ lvm ]
              state: present
          - name: setup vg
            lvg:
              vg: mysqlorchestrator
              pvs: /dev/vdb1
              pesize: 32
          - name: setup thing pool
            lvol:
              vg: mysqlorchestrator
              thinpool: thin
              size: 100%FREE
            when:
              - ansible_lvm['lvs']['thin'] is not defined
          - name: refresh facts
            setup:
          - name: setup lv
            lvol:
              vg: mysqlorchestrator
              lv: instance1
              thinpool: thin
              size: "{{ ansible_lvm['lvs']['thin']['size_g'] }}G"
            when:
              - ansible_lvm['lvs']['instance1'] is not defined
          - name: make filesystem
            filesystem:
              fstype: xfs
              dev: /dev/mapper/mysqlorchestrator-instance1
          - name: mount fs
            mount:
              path: ${mysql_datadir}
              src: /dev/mapper/mysqlorchestrator-instance1
              fstype: xfs
              opts: defaults,noatime
              state: mounted

      - name: bootstrap phase
        hosts: 127.0.0.1
        tasks:
          - name: bootstrap phase - create consul session
            uri:
              url: http://${consul}:${consul_port}/v1/session/create
              method: PUT
              status_code: 200
              body_format: json
            register: bootstrap_session_consul
            until: bootstrap_session_consul.failed == false
            delay: 1
            retries: 6000
          - set_fact:
              bootstrap_session: "{{ bootstrap_session_consul.json | json_query('ID') }}" 
          - name: bootstrap phase - aquire consul lock
            uri:
              url: "http://${consul}:${consul_port}/v1/kv/mysql/master/${name}/custom_bootstrap?acquire={{ bootstrap_session }}"
              method: PUT
              status_code: 200
              body: '{ name: ${hostname}.node.${consul_datacenter}.consul }'
            register: bootstrap_consul
            until: bootstrap_consul.failed == false
            delay: 1
            retries: 6000
          - set_fact:
              bootstrap: "{{ bootstrap_consul.json | bool }}" 

      - name: generate and spread ssh keys
        hosts: 127.0.0.1
        tasks:
          - block:
              - name: generate ssh keys
                shell: ssh-keygen -b 2048 -t rsa -f /tmp/id_rsa -q -N "" -P ${mysql_admin_password}
                args:
                  creates: /tmp/id_rsa

              - name: put private ssh key
                uri:
                  url: "http://${consul}:${consul_port}/v1/kv/mysql/master/${name}/custom_ssh_key"
                  method: PUT
                  status_code: 200
                  body: "{{ lookup('file', '/tmp/id_rsa') }}"
                register: ssh_key_consul
                until: ssh_key_consul.failed == false
                delay: 1
                retries: 6000

              - name: put public ssh key
                uri:
                  url: "http://${consul}:${consul_port}/v1/kv/mysql/master/${name}/custom_ssh_key_pub"
                  method: PUT
                  status_code: 200
                  body: "{{ lookup('file', '/tmp/id_rsa.pub') }}"
                register: ssh_key_pub_consul
                until: ssh_key_pub_consul.failed == false
                delay: 1
                retries: 6000

              - name: remove keys from temp dir
                file:
                  path: "{{ item }}"
                  state: absent
                with_items:
                  - /tmp/id_rsa
                  - /tmp/id_rsa.pub
            when: bootstrap == true

          - name: get private ssh key
            uri:
              url: http://${consul}:${consul_port}/v1/kv/mysql/master/${name}/custom_ssh_key
              method: GET
              status_code: 200
              body_format: json
            register: ssh_key_consul
            until: ssh_key_consul.failed == false
            delay: 1
            retries: 6000

          - name: write private encrypted ssh key
            copy:
              content: "{{ ssh_key_consul.json[0] | json_query('Value') | b64decode }}"
              dest: /root/.ssh/id_rsa.enc
              mode: 0400

          - name: decrypt ssh keys
            shell: openssl rsa -in /root/.ssh/id_rsa.enc -out /root/.ssh/id_rsa -passin pass:${mysql_admin_password} && chmod 400 /root/.ssh/id_rsa
            args:
              creates: /root/.ssh/id_rsa

          - name: get public ssh key
            uri:
              url: http://${consul}:${consul_port}/v1/kv/mysql/master/${name}/custom_ssh_key_pub
              method: GET
              status_code: 200
              body_format: json
            register: ssh_key_pub_consul
            until: ssh_key_pub_consul.failed == false
            delay: 1
            retries: 6000

          - name: write public ssh key
            copy:
              content: "{{ ssh_key_pub_consul.json[0] | json_query('Value') | b64decode }}"
              dest: /root/.ssh/id_rsa.pub
              mode: 0400

          - name: enable ssh key for direct root access
            authorized_key:
              user: root
              key: "{{ lookup('file', '/root/.ssh/id_rsa.pub') }}"

      - name: backup
        hosts: 127.0.0.1
        pre_tasks:
          - set_fact:
              restic_binary_path: /usr/local/bin
          - name: install backup requirements
            package: name="{{ item }}"
            with_items:
              - mylvmbackup
              - jq
              - libfile-copy-recursive-perl
          - name: overwrite preflush.pm
            copy:
              content: |
                #!/usr/bin/perl -w

                package preflush;

                use strict;

                my $errstr;

                system('/usr/share/mylvmbackup/preflush')

                sub execute()
                {
                        my ($class, $dbh, $msg) = @_;

                        if(!$dbh->do("FLUSH LOGS"))
                        {
                                $errstr = "Unable to flush logs: " . $DBI::errstr;
                                return 0;
                        }

                        return 1;
                }

                sub errmsg()
                {
                        return $errstr;
                }

                1;
              dest: /usr/share/mylvmbackup/preflush.pm
              mode: 0755
          - name: /usr/share/mylvmbackup/preflush
            copy:
              content: |
                #!/bin/bash

                killqueries () {
                    TIMEEND=5
                    TIMESTART=$(date +%s)
                    TIMEELAPSED=0

                    echo save the pid in /tmp/killqueries

                    while [ $${TIMEELAPSED} -lt $${TIMEEND} ]; do
                        TIMEELAPSED=$(( $(date +%s) - $${TIMESTART} ))
                        sleep 1
                        echo waiting $(( $${TIMEEND} - $${TIMEELAPSED} )) before kill all queries
                    done

                    echo flush table is taking too long
                    echo kill all queries except FLUSH TABLES WITH READ LOCK
                    pt-kill --busy-time $${TIMEELAPSED} --kill --print --run-time 1 --interval 0 --ignore-user '^(replica|backup)$' --match-all --victim all
                }

                killqueries &
                echo $! > /tmp/killqueries
              dest: /usr/share/mylvmbackup/preflush
              mode: 0755
          - name: /usr/share/mylvmbackup/predisconnect
            copy:
              content: |
                #!/bin/bash
                KILL=$(kill $(cat /tmp/killqueries) 2>&1)
                RCKILL=$?
                echo $KILL | grep "No such process" > /dev/null 2>&1
                NOPROCESS=$?
                if [ "$RC" != "0" ] && [ "$NOPROCESS" == "0" ]; then
                    echo pt-kill terminated before kill
                    exit 0
                fi
                echo $KILL
                exit $RCKILL
              dest: /usr/share/mylvmbackup/predisconnect
              mode: 0755
        roles:
          - role: entercloudsuite.backup
            customer: "${os_project}"
            Customer_MaxSize: "524288000"
            restic_repository: swift:mysql_${name}:/
            restic_repository_password: ${mysql_admin_password}
            restic_forget_time:
              '--keep-daily': '${restic_forget_time_day}'
            restic_backup_path: /var/cache/mylvmbackup/mnt/backup
            restic_start_backup_time: "${restic_start_backup_time}"
            influxDB_url: "${influxdb_url}"
            influxDB_port: "${influxdb_port}"
            influxDB_DatabaseName: "${influxdb_databasename}"
            influxDB_Username: "${influxdb_username}"
            influxDB_Password: "${influxdb_password}"
            os_api: "${os_api}"
            os_region: "${os_region}"
            os_project: "${os_project}"
            os_project_id: "${os_project_id}"
            os_user: "${os_user}"
            os_password: "${os_password}"
            restic_setup_repository: false
            restic_functions:
              - backup
              - forget
              - prune
            restic_backup_script: |
              master=$(curl -s ${consul}:${consul_port}/v1/kv/mysql/master/${name}/hostname -s | jq '.[].Value' | sed 's/"//g' | base64 -d )
              if [ "$master" = "${hostname}.node.${consul_datacenter}.consul" ] || [ "$1" = "force" ]; then
                  echo "lvchange -ay -Ky /dev/mysqlorchestrator/instance1_snapshot" > /usr/share/mylvmbackup/premount
                  chmod +x /usr/share/mylvmbackup/premount
                  echo "cp /tmp/mastergtid /var/cache/mylvmbackup/mnt/backup/mastergtid" > /usr/share/mylvmbackup/prebackup
                  chmod +x /usr/share/mylvmbackup/prebackup

                  cat <<'EOF' > /usr/share/mylvmbackup/preunlock
              #!/bin/bash
              function get_gtid_executed()
              {
                  local count
                  local res

                  count=0
                  while read line; do
                      if [ $count -eq 5 ] # File:
                      then
                          res=`echo "$line" | sed s/Executed_Gtid_Set://`
                          break;
                      fi
                      count=$((count+1))
                  done <<< "`mysql -Nse 'SHOW MASTER STATUS\G' mysql`"

                  echo $res
              }

              get_gtid_executed > /tmp/mastergtid
              EOF
                  chmod +x /usr/share/mylvmbackup/preunlock

                  mylvmbackup --user=backup --password=backup --mycnf=/etc/mysql/my.cnf --vgname=mysqlorchestrator --lvname=instance1 --innodb_recover --thin --xfs --backuptype=none --recoveryopts '--skip-networking --bootstrap --skip-grant-tables --skip-syslog --skip-slave-start' --keep_snapshot --keep_mount
              else
                  exit 0
              fi
            restic_after_prune_script: |
              umount /var/cache/mylvmbackup/mnt/backup
              lvremove -y mysqlorchestrator/instance1_snapshot

      - name: copy master mysql data
        hosts: 127.0.0.1
        pre_tasks:
          - name: check that the dir ${mysql_datadir}/mysql exist
            stat:
              path: ${mysql_datadir}/mysql
            register: _mysql_datadir
          - name: get mysql master from consul
            block:
              - name: get master hostname
                uri:
                  url: http://${consul}:${consul_port}/v1/kv/mysql/master/${name}/hostname
                  method: GET
                  status_code: 200
                  body_format: json
                register: mysql_master_consul
                until: mysql_master_consul.failed == false
                delay: 1
                retries: 6000
              - name: get master port
                uri:
                  url: http://${consul}:${consul_port}/v1/kv/mysql/master/${name}/port
                  method: GET
                  status_code: 200
                  body_format: json
                register: mysql_master_port_consul
                until: mysql_master_port_consul.failed == false
                delay: 1
                retries: 6000
              - set_fact:
                  mysql_master: "{{ mysql_master_consul.json | json_query('[0].Value') | b64decode }}" 
              - set_fact:
                  mysql_master_port: "{{ mysql_master_port_consul.json | json_query('[0].Value') | b64decode }}" 
              - name: wait until ssh and mysql is available on "{{ mysql_master }}:{{ mysql_master_port }}"
                wait_for:
                  host: "{{ mysql_master }}"
                  port: "{{ item }}"
                  timeout: 6000
                with_items:
                  - 22
                  - "{{ mysql_master_port }}"
            when: ( bootstrap == false)
          - name: restore latest mysql data release
            block:
              - name: restore data
                shell: |
                  source /usr/local/restic/customer-data.sh
                  export RESTIC_REPOSITORY="swift:mysql_${name}:/"
                  export RESTIC_PASSWORD="${mysql_admin_password}"
                  export XDG_CACHE_HOME="/var/backups/restic_cache"
                  restic restore latest --target ${mysql_datadir}/.restore
                  mv ${mysql_datadir}/.restore/var/cache/mylvmbackup/mnt/backup/*  ${mysql_datadir}/
                  rm -rf ${mysql_datadir}/.restore
                  rm ${mysql_datadir}/auto.cnf
                args:
                  executable: /bin/bash
            when: ( bootstrap == false ) and
                  ( _mysql_datadir.stat.exists == false )
          - name: set mysql owner to ${mysql_datadir}
            file: path=${mysql_datadir} recurse=yes owner=mysql group=mysql
          - name: setup client password
            copy:
              dest: /root/.my.cnf
              content: |
                [client]
                user=${mysql_root_name}
                password=${mysql_root_password}

      - name: configure mysql
        hosts: 127.0.0.1
        pre_tasks:
          - name: stop mysql
            service: name=mysql state=stopped
          - name: clean up old installation
            file: name=/var/lib/mysql state=absent
          - name: add custom mysql config file
            copy:
              dest: /etc/mysql/conf.d/custom.cnf
              content: |
                [mysqld]
                # user options
                ${mysql_user_options}
                # read only options
                read_only
                super_read_only
                port                              = ${mysql_port}
                datadir                           = ${mysql_datadir}
                max_connections                   = 60000
                innodb_buffer_pool_size           = "{{ (ansible_memtotal_mb*0.8)|int|abs }}M"
                server-id                         = "{{ 4294967294 | random }}"
                innodb_flush_log_at_trx_commit    = 2
                sync_binlog                       = 0
                log_bin                           = mysql-bin
                log-bin-index                     = mysql-bin.index
                innodb_log_file_size              = 512M
                expire_logs_days                  = 3
                max_binlog_size                   = 256M
                binlog_format                     = ROW
                enforce_gtid_consistency          = ON
                gtid_mode                         = ON
                report-host                       = ${hostname}.node.${consul_datacenter}.consul
                log-slave-updates                 = 1
                relay_log_info_repository         = TABLE
                master_info_repository            = TABLE
                slave_parallel_type               = LOGICAL_CLOCK
                slave_parallel_workers           = "{{ ansible_processor_vcpus }}"
                # pmm options
                log_output                        = file
                slow_query_log                    = ON
                long_query_time                   = 0
                log_slow_rate_limit               = 100
                log_slow_rate_type                = query
                log_slow_verbosity                = full
                log_slow_admin_statements         = ON
                log_slow_slave_statements         = ON
                slow_query_log_always_write_time  = 1
                slow_query_log_use_global_control = all
                innodb_monitor_enable             = all
                userstat                          = 1
          - name: startup mysql
            service: name=mysql state=started
          - name: set super read only off
            shell:
              cmd: |
                mysql -uroot -p${mysql_root_password} -e "set global super_read_only=false"
          - name: upgrade mysql system tables
            shell:
              cmd: |
                mysql_upgrade -s
            register: mysql_upgrade
            failed_when: mysql_upgrade.rc != 0 and mysql_upgrade.rc != 2
          - name: set super read only off
            shell:
              cmd: |
                mysql -uroot -p${mysql_root_password} -e "set global super_read_only=true"
        post_tasks:
          - block:
              - name: set read only off
                shell:
                  cmd: |
                    mysql -uroot -p${mysql_root_password} -e "set global read_only=false"
              - name: set super read only off
                shell:
                  cmd: |
                    mysql -uroot -p${mysql_root_password} -e "set global super_read_only=false"
              - name: create mysql admin ${mysql_admin_name}
                mysql_user:
                  name: ${mysql_admin_name}
                  host: '%'
                  password: ${mysql_admin_password}
                  priv: '*.*:ALL,GRANT'
              - name: create mysql backup user
                mysql_user:
                  name: backup
                  host: 'localhost'
                  password: backup
                  priv: '*.*:ALL'
              - name: create mysql user ${mysql_replica_user_name}
                mysql_user:
                  name: ${mysql_replica_user_name}
                  host: '%'
                  password: ${mysql_replica_user_password}
                  priv: '*.*:REPLICATION SLAVE,REPLICATION CLIENT'
              - name: create mysql user orchestrator_${orchestrator_user}
                mysql_user:
                  name: orchestrator_${orchestrator_user}
                  host: '%'
                  password: ${orchestrator_password}
                  priv: '*.*:RELOAD,PROCESS,SUPER,REPLICATION SLAVE,REPLICATION CLIENT'
              - name: add permission to read mysql.slave_master_info to mysql user orchestrator_${orchestrator_user}
                mysql_user:
                  name: orchestrator_${orchestrator_user}
                  host: '%'
                  password: orchestrator_${orchestrator_password}
                  priv: 'mysql.slave_master_info:SELECT'
                  append_privs: yes
              - name: add permission to read meta.* to mysql user orchestrator_${orchestrator_user}
                mysql_user:
                  name: orchestrator_${orchestrator_user}
                  host: '%'
                  password: ${orchestrator_password}
                  priv: 'meta.*:SELECT'
                  append_privs: yes
              - name: setup meta table
                shell:
                  cmd: |
                    mysql -uroot -p${mysql_root_password} << EOF
                      CREATE DATABASE IF NOT EXISTS meta;
                      CREATE TABLE IF NOT EXISTS meta.cluster (
                        id int NOT NULL,
                        cluster_name varchar(255),
                        cluster_domain varchar(255),
                        anchor bool,
                        PRIMARY KEY (id)
                      );
                      INSERT INTO meta.cluster(id, cluster_name, anchor) VALUES (1, '${name}', 1)
                      ON DUPLICATE KEY UPDATE cluster_name = '${name}';
                    EOF
            when: bootstrap == true


          - name: check that the slave coordination file exist
            stat:
              path: ${mysql_datadir}/mastergtid
            register: _mastergtid
          - name: set up replica
            block:
              - name: set read only at runtime
                shell: >
                  mysql -uroot -p${mysql_root_password} -e "set global read_only=true"
              - name: get gtid position
                shell: cat ${mysql_datadir}/mastergtid
                register: _gtid_position
              - name: reset master status on slave node
                shell: >
                  mysql -uroot -p${mysql_root_password} -e "RESET MASTER"
              - name: set lastest purged gtid
                shell: >
                  mysql -uroot -p${mysql_root_password} -e "SET GLOBAL gtid_purged=\"{{ _gtid_position.stdout }}\";"
              - name: target master server
                shell: >
                  mysql -uroot -p${mysql_root_password} -e "CHANGE MASTER TO MASTER_HOST=\"{{ mysql_master }}\", MASTER_PORT={{ mysql_master_port }}, MASTER_USER=\"${mysql_replica_user_name}\", MASTER_PASSWORD=\"${mysql_replica_user_password}\", MASTER_AUTO_POSITION = 1;"
              - name: start slave
                shell: >
                  mysql -uroot -p${mysql_root_password} -e "START SLAVE\G;"
              - name: delete ${mysql_datadir}/mastergtid file
                file: path=${mysql_datadir}/mastergtid state=absent
            when: ( bootstrap == false ) and
                  ( _mastergtid.stat.exists == True )

      - name: init backup
        hosts: 127.0.0.1
        tasks:
          - block:
              - name: create restic repository
                shell: |
                  source /usr/local/restic/customer-data.sh
                  export RESTIC_REPOSITORY="swift:mysql_${name}:/"
                  export RESTIC_PASSWORD="${mysql_admin_password}"
                  {{ restic_binary_path }}/restic init
                args:
                  executable: /bin/bash
                ignore_errors: yes
              - name: create first backup
                shell: /bin/bash -x /usr/local/restic/files_backup.sh force &> /var/log/cloud-firstbackup.log
                args:
                  executable: /bin/bash
            when: bootstrap == true

      - name: register node
        hosts: 127.0.0.1
        tasks:
          - name: register host to orchestrator
            uri:
              url: http://${orchestrator}:${orchestrator_port}/api/discover/${hostname}.node.${consul_datacenter}.consul/${mysql_port}
              method: GET
              status_code: 200
              body_format: json
              user: ${orchestrator_user}
              password: ${orchestrator_password}
              force_basic_auth: yes
            register: register_orchestrator
            until: register_orchestrator.failed == false
            delay: 1
            retries: 6000

          - block:
              - name: update consul with master the new cluster
                shell: |
                  curl -sS http://${orchestrator_user}:${orchestrator_password}@${orchestrator}:${orchestrator_port}/api/submit-masters-to-kv-stores > /dev/null
                  curl -f -sS http://${consul}:${consul_port}/v1/kv/mysql/master/${name}/hostname
                register: update_consul
                until: ( update_consul.rc == 0 ) and
                   ( update_consul.stdout | from_json | json_query('[0].Value') | b64decode == "${hostname}.node.${consul_datacenter}.consul" )
                delay: 1
                retries: 6000
            when: bootstrap == true


      - name: metrics percona
        hosts: 127.0.0.1
        tasks:
          - block:
              - name: deregister host to pmm
                uri:
                  url: http://${pmm_server}/v1/catalog/deregister?dc=dc1
                  method: PUT
                  status_code: 200
                  body_format: json
                  user: ${pmm_user}
                  password: ${pmm_password}
                  force_basic_auth: yes
                  body: '{"Datacenter":"dc1","Node":"${hostname}"}'
                register: deregister_pmm
                until: deregister_pmm.failed == false
                delay: 1
                retries: 3
                ignore_errors: True
              - name: systemd unit for connect pmm client to server
                copy:
                  dest: /usr/src/cloud/metricspercona.yml
                  mode: 0550
                  content: |
                    - name: pmm_client
                      hosts: 127.0.0.1
                      roles:
                        - role: entercloudsuite.pmm_client
                          pmm_client_server_host: ${pmm_server}
                          pmm_client_server_port: 443
                          pmm_client_server_basic_auth: True
                          pmm_client_server_use_ssl: True
                          pmm_client_server_basic_auth_username: ${pmm_user}
                          pmm_client_server_basic_auth_password: ${pmm_password}
                          pmm_client_add_services:
                            - linux:metrics
                            - mysql:metrics
                            - mysql:queries
                          pmm_client_start_services:
                            - linux:metrics
                            - mysql:metrics
                            - mysql:queries
                          pmm_client_db:
                            mysql:
                              host: localhost
                              port: ${mysql_port}
                              username: root
                              password: root
                      post_tasks:
                        - name: clean up orphan agents
                          shell: pmm-admin repair
              - name: systemd unit for connect pmm client to server
                copy:
                  dest: /etc/systemd/system/metricspercona.service
                  mode: 0550
                  content: |
                    [Unit]
                    Description=Connect pmm client to server
                    Requires=network-online.target
                    After=network.target

                    [Service]
                    ExecStart=/usr/src/cloud/venv/bin/ansible-playbook -e ansible_python_interpreter=/usr/bin/python --connection=local /usr/src/cloud/metricspercona.yml
                    TimeoutStopSec=5

                    [Install]
                    WantedBy=multi-user.target
              - name: systemd unit timer for connect pmm client to server
                copy:
                  dest: /etc/systemd/system/metricspercona.timer
                  mode: 0550
                  content: |
                    [Unit]
                    Description=Run metricspercona

                    [Timer]
                    OnCalendar=*:0/5
                    Persistent=true
                    Unit=metricspercona.service

                    [Install]
                    WantedBy=multi-user.target
              - name: reload systemd
                systemd:
                  name: metricspercona.timer
                  daemon_reload: yes
                  enabled: yes
                  state: started
            when: '"${pmm_server}" != ""'

    path: /usr/src/cloud/playbook.yml
    permissions: '0400'

runcmd:
  - |
      bash <<'EOF'
      # Run main playbook
      export COMPLETED=false
      while [ "$COMPLETED" == "false" ]; do
        (
          cd /usr/src/cloud
          source venv/bin/activate
          ansible-playbook -e ansible_python_interpreter=/usr/bin/python --connection=local playbook.yml
        ) >> /var/log/cloud-scripts.log 2>&1
        if [ $? == 0 ]; then
          COMPLETED=true
        fi
        sleep 1
      done
      export COMPLETED=false
      EOF
