#cloud-config

write_files:
  - content: |
      - name: install orchestrator
        hosts: 127.0.0.1
        vars:
          orchestrator_http_auth_basic: '${orchestrator_http_auth_user}:${orchestrator_http_auth_password}'
        pre_tasks:
           - name: stop consul
             service: name=consul state=stopped
           - name: clean /opt/consul/data/serf/local.keyring
             file: path=/opt/consul/data/serf/local.keyring state=absent
        roles:
          - role: entercloudsuite.orchestrator
            orchestrator_mysql_user: orchestrator
            orchestrator_mysql_password: orchestrator
            orchestrator_mysql_topology_user: orchestrator_${orchestrator_user}
            orchestrator_mysql_topology_password: ${orchestrator_password}
            orchestrator_read_only: false
            orchestrator_super_read_only: true
            orchestrator_mysql_compatible_version:
            orchestrator_listen_address: :${orchestrator_service_port}
            orchestrator_kv_consul_address: ${consul}:${consul_port}
            orchestrator_recover_master_cluster_filters: ["*"]
            orchestrator_recover_intermediate_master_cluster_filters: ["*"]
            orchestrator_master_failover_lost_instances_downtime_minutes: 1
            orchestrator_failure_detection_period_block_minutes: 1
            orchestrator_recovery_period_block_seconds: 30
            orchestrator_detect_cluster_domain_query:
            orchestrator_detect_datacenter_query:
            orchestrator_replication_lag_query:
            orchestrator_pseudo_gtid_pattern:
            orchestrator_detect_pseudo_gtid_query:
            orchestrator_authentication_method: ${orchestrator_authentication_method}
            orchestrator_http_auth_user: ${orchestrator_http_auth_user}
            orchestrator_http_auth_password: ${orchestrator_http_auth_password}
            orchestrator_url_prefix:
            orchestrator_raft_enabled: ${orchestrator_raft_enabled}
            orchestrator_raft_data_dir: "${orchestrator_raft_data_dir}"
            orchestrator_raft_bind: "{{ ansible_default_ipv4.address }}"
            orchestrator_raft_default_port: ${orchestrator_raft_default_port}
            orchestrator_raft_nodes: ${orchestrator_raft_nodes}
          - role: entercloudsuite.haproxy
            haproxy_user: ${orchestrator_user}
            haproxy_pass: ${orchestrator_password}
            haproxy_conf: |
              resolvers dns-consul
                  nameserver dns consul.service.${consul_datacenter}.consul:53
                  accepted_payload_size 8192
                  hold valid 1s

              listen orchestrator
                  bind  *:${orchestrator_port}
                  mode tcp
                  option httpchk GET /api/leader-check HTTP/1.0\r\nAuthorization:\ Basic\ {{ orchestrator_http_auth_basic | b64encode }}
                  balance first
                  retries 1
                  timeout connect 1000
                  timeout check 300
                  timeout server 30s
                  timeout client 30s
                  default-server port 3000 fall 1 inter 1000 rise 1 downinter 1000 on-marked-down shutdown-sessions weight 10 init-addr last,libc,none resolvers dns-consul
                  server ${name}-0 ${name}-0.node.${consul_datacenter}.consul:${orchestrator_service_port} check
                  server ${name}-1 ${name}-1.node.${consul_datacenter}.consul:${orchestrator_service_port} check
                  server ${name}-2 ${name}-2.node.${consul_datacenter}.consul:${orchestrator_service_port} check
          - role: entercloudsuite.consul
            consul_config_validate: "{{ consul_user_home }}/bin/consul validate -config-format=json %s"
            consul_configs:
              main:
                bind_addr: 0.0.0.0
                client_addr: 0.0.0.0
                node_name: "{{ ansible_hostname }}"
                data_dir: "{{ consul_data_dir }}"
                encrypt: "${consul_encrypt}"
                datacenter: "${consul_datacenter}"
                enable_syslog: true
                server: false
                ui: true
                enable_script_checks: true
                services:
                  - name: "${name}"
                    checks:
                      - http: "http://${orchestrator_user}:${orchestrator_password}@127.0.0.1:8282"
                        method: "GET"
                        interval: "2s"
                  - name: "exporter_node"
                    port: 9100
                  - name: "exporter_haproxy"
                    port: 9101
                  - name: "exporter_mysqld"
                    port: 9104
                rejoin_after_leave: true
                retry_join:
                  - "${consul}"
            when: ("${consul}" != "")
        post_tasks:
          - name: force update master every minute
            cron:
              name: "force update master every minute"
              minute: "*/1"
              job: "curl -sS http://${orchestrator_user}:${orchestrator_password}@${name}.service.${consul_datacenter}.consul/api/submit-masters-to-kv-stores > /dev/null"

      - name: get orchestrator leader
        hosts: 127.0.0.1
        tasks:
          - block:
              - name: set bootstrap default
                set_fact:
                  bootstrap: false
              - name: wait leader election
                uri:
                  url: http://${orchestrator_user}:${orchestrator_password}@${name}.service.${consul_datacenter}.consul/api/raft-state
                  status_code: 200
                  body_format: json
                register: orchestrator_api_leader_elected
                until: ( orchestrator_api_leader_elected.failed == false ) and
                  ( orchestrator_api_leader_elected.json == "Leader" )
                delay: 1
                retries: 6000
              - name: get leader node
                uri:
                  url: http://${orchestrator_user}:${orchestrator_password}@${name}.service.${consul_datacenter}.consul/api/raft-leader
                  status_code: 200
                  body_format: json
                register: orchestrator_api_leader
                until: orchestrator_api_leader.failed == false
                delay: 1
                retries: 6000
              - name: set leader
                set_fact:
                  orchestrator_leader: "{{ orchestrator_api_leader.json.split(':')[0] | regex_replace('\"','') }}"
              - name: set bootstrap
                set_fact:
                  bootstrap: true
                when: orchestrator_leader == ansible_default_ipv4.address

      - name: generate and spread ssh keys
        hosts: 127.0.0.1
        tasks:
          - block:
              - name: generate ssh keys
                shell: ssh-keygen -b 2048 -t rsa -f /tmp/id_rsa -q -N "" -P ${orchestrator_password}
                args:
                  creates: /tmp/id_rsa
              - name: put private ssh key
                uri:
                  url: "http://${consul}:${consul_port}/v1/kv/orchestrator/master/${name}/custom_ssh_key"
                  method: PUT
                  status_code: 200
                  body: "{{ lookup('file', '/tmp/id_rsa') }}"
                register: ssh_key_consul
                until: ssh_key_consul.failed == false
                delay: 1
                retries: 6000
              - name: put public ssh key
                uri:
                  url: "http://${consul}:${consul_port}/v1/kv/orchestrator/master/${name}/custom_ssh_key_pub"
                  method: PUT
                  status_code: 200
                  body: "{{ lookup('file', '/tmp/id_rsa.pub') }}"
                register: ssh_key_pub_consul
                until: ssh_key_pub_consul.failed == false
                delay: 1
                retries: 6000
              - name: remove keys from temp dir
                file:
                  path: "{{ item }}"
                  state: absent
                with_items:
                  - /tmp/id_rsa
                  - /tmp/id_rsa.pub
            when: bootstrap == true
          - name: get private ssh key
            uri:
              url: http://${consul}:${consul_port}/v1/kv/orchestrator/master/${name}/custom_ssh_key
              method: GET
              status_code: 200
              body_format: json
            register: ssh_key_consul
            until: ssh_key_consul.failed == false
            delay: 1
            retries: 6000
          - name: write private encrypted ssh key
            copy:
              content: "{{ ssh_key_consul.json[0] | json_query('Value') | b64decode }}"
              dest: /root/.ssh/id_rsa.enc
              mode: 0400
          - name: decrypt ssh keys
            shell: openssl rsa -in /root/.ssh/id_rsa.enc -out /root/.ssh/id_rsa -passin pass:${orchestrator_password} && chmod 400 /root/.ssh/id_rsa
            args:
              creates: /root/.ssh/id_rsa
          - name: get public ssh key
            uri:
              url: http://${consul}:${consul_port}/v1/kv/orchestrator/master/${name}/custom_ssh_key_pub
              method: GET
              status_code: 200
              body_format: json
            register: ssh_key_pub_consul
            until: ssh_key_pub_consul.failed == false
            delay: 1
            retries: 6000
          - name: write public ssh key
            copy:
              content: "{{ ssh_key_pub_consul.json[0] | json_query('Value') | b64decode }}"
              dest: /root/.ssh/id_rsa.pub
              mode: 0400
          - name: enable ssh key for direct root access
            authorized_key:
              user: root
              key: "{{ lookup('file', '/root/.ssh/id_rsa.pub') }}"

      - name: sync orchestrator node from a health raft node
        hosts: 127.0.0.1
        pre_tasks:
          - block:
              - name: wait ssh to be reachable on leader node
                wait_for:
                  port: 22
                  host: ${name}.service.${consul_datacenter}.consul
                  search_regex: OpenSSH
                  timeout: 6000
              - name: wait mysql to be reachable on leader node
                wait_for:
                  port: 3306
                  host: ${name}.service.${consul_datacenter}.consul
                  timeout: 6000
                connection: local
              - name: stop orchestrator
                service: name=orchestrator state=stopped
              - name: stop mysql
                service: name=mysql state=stopped
              - name: clean content of /var/lib/mysql
                file: path=/var/lib/mysql/ state=absent
            when: bootstrap == false
        roles:
          - role: entercloudsuite.mysql-innobackupex
            innobackupex_source_server: "{{ orchestrator_leader }}"
            innobackupex_destination: /var/lib/mysql
            when: bootstrap == false
        post_tasks:
          - block:
              - name: set mysql permission
                file: dest=/var/lib/mysql owner=mysql group=mysql recurse=yes
              - name: start mysql
                service: name=mysql state=started
              - name: start orchestrator
                service: name=orchestrator state=started
            when: bootstrap == false
    path: /usr/src/cloud/playbook.yml
    permissions: '0400'

runcmd:
  - |
      bash <<'EOF'
      export COMPLETED=false
      while [ "$COMPLETED" == "false" ]; do
        (
          set -e errexit
          set -o pipefail
          # workaround https://github.com/ansible/ansible/issues/21562
          export HOME=/root
          cd /usr/src/cloud
          source venv/bin/activate
          ansible-playbook -e ansible_python_interpreter=/usr/bin/python --connection=local playbook.yml
        ) >> /var/log/cloud-scripts.log 2>&1
        if [ $? == 0 ]; then
          COMPLETED=true
        fi
        sleep 1
      done
      EOF
